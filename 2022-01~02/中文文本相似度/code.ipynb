{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务1：报名比赛，下载比赛数据集并完成读取\n",
    "\n",
    "- 步骤1 ：登录&报名比赛：https://aistudio.baidu.com/aistudio/competition/detail/45/0/task-definition\n",
    "- 步骤2 ：下载比赛数据集\n",
    "- 步骤3 ：使用Pandas完成数据读取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error uploading: check_hostname requires server_hostname\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.2.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "import distance \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False #用来正常显示负号\n",
    "\n",
    "pal = sns.color_palette()\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:/study_hard/statistic/千言数据集'\n",
    "data_list = ['bq_corpus','lcqmc','paws-x-zh']\n",
    "# data_dir = 'E:/学习/千言数据集/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                            q1                q2 label\n",
       " 0             用微信都6年，微信没有微粒贷功能          4。号码来微粒贷     0\n",
       " 1                       微信消费算吗           还有多少钱没还     0\n",
       " 2         交易密码忘记了找回密码绑定的手机卡也掉了  怎么最近安全老是要改密码呢好麻烦     0\n",
       " 3  你好我昨天晚上申请的没有打电话给我今天之内一定会打吗？          什么时候可以到账     0\n",
       " 4                      “微粒贷开通\"   你好，我的微粒贷怎么没有开通呢     0,\n",
       " (86198, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先读一个数据集，后面统一读\n",
    "\n",
    "train = pd.read_csv(data_dir+'/bq_corpus/train.tsv',sep='\\t',error_bad_lines=False,names=['q1','q2','label']).dropna()\n",
    "test = pd.read_csv(data_dir+'/bq_corpus/test.tsv',sep='\\t',error_bad_lines=False,names=['q1','q2']).dropna()\n",
    "test['label'] = -1 \n",
    "dev = pd.read_csv(data_dir+'/bq_corpus/dev.tsv',sep='\\t',error_bad_lines=False,names=['q1','q2','label']).dropna()\n",
    "\n",
    "train.head(),train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务2：对句子对提取TFIDF以及统计特征，训练和预测\n",
    "\n",
    "参考代码：https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb\n",
    "- 步骤1 ：对句子对（句子A和句子B统计）如下特征：\n",
    "    - 句子A包含的字符个数、句子B包含的字符个数\n",
    "    - 句子A与句子B的编辑距离\n",
    "    - 句子A与句子B共有单词的个数\n",
    "    - 句子A与句子B共有字符的个数\n",
    "    - 句子A与句子B共有单词的个数 / 句子A字符个数\n",
    "    - 句子A与句子B共有单词的个数 / 句子B字符个数\n",
    "- 步骤2 ：计算TFIDF，并对句子A和句子B进行特征转换，并进行\n",
    "- 步骤3 ：计算句子A与句子B的TFIDF向量的内积距离\n",
    "- 步骤4 ：将上述特征送入分类模型，训练并预测，将结果预测提交到比赛网站。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\CEALLA~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model from cache C:\\Users\\CEALLA~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.551 seconds.\n",
      "Loading model cost 0.551 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 句子对特征\n",
    "\n",
    "# 字符个数\n",
    "train['q1_len'] = train['q1'].apply(len)\n",
    "train['q2_len'] = train['q2'].apply(len)  \n",
    "\n",
    "# 编辑距离 \n",
    "# Levenshtein Distance 被称为编辑距离（Edit Distance），一个度量两个字符序列之间差异的字符串度量标准\n",
    "train['Lev_distance'] = train.apply(lambda x:distance.levenshtein(x['q1'],x['q2']),axis=1)\n",
    "\n",
    "## jieba分词 \n",
    "# cut_all=True，全模式，“我来到北京清华大学”-->“ 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学”\n",
    "def jieba_cut(sentence):\n",
    "    word_list = jieba.lcut(sentence,cut_all=True) \n",
    "    return word_list \n",
    "\n",
    "train['q1_cut'] = train['q1'].apply(lambda x:jieba_cut(x)) \n",
    "train['q2_cut'] = train['q2'].apply(lambda x:jieba_cut(x)) \n",
    "\n",
    "# 分词后的词个数\n",
    "train['q1_cut_len'] = train['q1_cut'].apply(len)\n",
    "train['q2_cut_len'] = train['q2_cut'].apply(len)  \n",
    "   \n",
    "# 分词后，两句子相同词占所有词（去重）的比例\n",
    "def percent(q1_cut,q2_cut):\n",
    "    inter_num = len(set(q1_cut) & set(q2_cut))\n",
    "    percent = inter_num/len(set(q1_cut))\n",
    "    return percent\n",
    "\n",
    "train['q1_cut_percent'] = train.apply(lambda x: percent(x['q1_cut'],x['q2_cut']),axis=1)\n",
    "train['q2_cut_percent'] = train.apply(lambda x: percent(x['q2_cut'],x['q1_cut']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 获取停词 \n",
    "# https://github.com/goto456/stopwords\n",
    "\n",
    "def stopwords():\n",
    "    stop_words =[]\n",
    "    with open('cn_stopwords.txt','r',encoding='UTF-8') as f:\n",
    "        for i in f.readlines():\n",
    "            i = i.replace('\\n','')\n",
    "            stop_words.append(i)\n",
    "    return stop_words\n",
    "\n",
    "# 词共享 比例\n",
    "def word_match_share(row,stops):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    # 剔除停词\n",
    "    for word in str(row['q1_cut']):\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['q2_cut']):\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "\n",
    "\n",
    "train['word_match'] = train.apply(lambda x: word_match_share(x,stopwords()),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf 相似度\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# 准备语料\n",
    "def all_words(train):\n",
    "    corpus = []\n",
    "    # 遍历每行，q1分词，q2分词，合并\n",
    "    for row_id in range(len(train)):\n",
    "        row = train.iloc[row_id]\n",
    "        all_words = list()\n",
    "        all_words.extend([word for word in row['q1_cut'] if word not in stopwords()])\n",
    "        all_words.extend([word for word in row['q2_cut'] if word not in stopwords()])\n",
    "        corpus.append(' '.join(all_words))\n",
    "    return corpus\n",
    "\n",
    "corpus = all_words(train)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words and weights: \n",
      "\n",
      "[('还款', 2.6867275658248254e-05), ('微粒', 2.915026963999417e-05), ('贷', 2.9583173091145757e-05), ('借款', 3.2057446944925306e-05), ('电话', 3.6471060213720413e-05), ('没有', 3.787878787878788e-05), ('银行', 3.8825904643578196e-05), ('微', 4.675081813931744e-05), ('额度', 4.805151122002787e-05), ('你好', 5.1698288786641165e-05)]\n",
      "\n",
      "Least common words and weights: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('手机卡', 9.998000399920016e-05),\n",
       " ('门', 9.998000399920016e-05),\n",
       " ('装傻', 9.998000399920016e-05),\n",
       " ('私聊', 9.998000399920016e-05),\n",
       " ('看开', 9.998000399920016e-05),\n",
       " ('网点', 9.998000399920016e-05),\n",
       " ('退钱', 9.998000399920016e-05),\n",
       " ('察看', 9.998000399920016e-05),\n",
       " ('6624', 9.998000399920016e-05),\n",
       " ('算下来', 9.998000399920016e-05)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = (' '.join(corpus).split())\n",
    "word_cnt = Counter(corpus)\n",
    "\n",
    "# 定义权重\n",
    "# 词个数为1的，权重为0，大于1的，权重为 1/(count+10000)\n",
    "def get_weight(cnt, eps=10000, min_count=2):\n",
    "    if cnt < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (cnt + eps)\n",
    "    \n",
    "# 生成词权重，词--权重\n",
    "weight =  { word:get_weight(cnt) for word ,cnt in word_cnt.items()}\n",
    "\n",
    "print('Most common words and weights: \\n')\n",
    "print(sorted(weight.items(), key=lambda x: x[1] if x[1] > 0 else 9999)[:10])\n",
    "print('\\nLeast common words and weights: ')\n",
    "(sorted(weight.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_word_match_share(row,weight):\n",
    "    q1words = {word:1 for word in row['q1_cut'] if word not in stopwords()}\n",
    "    q2words = {word:1 for word in row['q2_cut'] if word not in stopwords()}\n",
    "    if len(q1words)==0 or len(q2words)==0:\n",
    "        return 0 \n",
    "    \n",
    "    # 获取共享词的权重\n",
    "    shared_weights = [weight.get(w,0) for w in q1words.keys() if w in q2words] + [weight.get(w,0) for w in q2words.keys() if w in q1words]\n",
    "    # 总权重\n",
    "    total_weights = [weight.get(w, 0) for w in q1words] + [weight.get(w, 0) for w in q2words]\n",
    "    # 共享词权重比例\n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "train['tfidf_word_match'] = train.apply(lambda x:tfidf_word_match_share(x,weight),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d86c190dfcadcdaa67edec4a1ea82702241987b5b1f320c920d3d4ca36fee5b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
